{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Redes Neurais Profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:01:52.822917Z",
     "start_time": "2021-01-18T10:00:57.391383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:01:52.844925Z",
     "start_time": "2021-01-18T10:01:52.839998Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização em lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:52:15.339683Z",
     "start_time": "2021-01-17T12:52:14.305517Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "training = tf.compat.v1.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.compat.v1.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.compat.v1.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.compat.v1.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name='hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name='hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name='outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:01:54.672633Z",
     "start_time": "2021-01-18T10:01:52.861851Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:52:54.976574Z",
     "start_time": "2021-01-17T12:52:16.010694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9002\n",
      "1 Validation accuracy: 0.9168\n",
      "2 Validation accuracy: 0.9328\n",
      "3 Validation accuracy: 0.9424\n",
      "4 Validation accuracy: 0.9478\n",
      "5 Validation accuracy: 0.952\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.9586\n",
      "8 Validation accuracy: 0.9612\n",
      "9 Validation accuracy: 0.9626\n",
      "10 Validation accuracy: 0.9642\n",
      "11 Validation accuracy: 0.966\n",
      "12 Validation accuracy: 0.9664\n",
      "13 Validation accuracy: 0.967\n",
      "14 Validation accuracy: 0.9698\n",
      "15 Validation accuracy: 0.9702\n",
      "16 Validation accuracy: 0.9712\n",
      "17 Validation accuracy: 0.9714\n",
      "18 Validation accuracy: 0.9704\n",
      "19 Validation accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:52:55.162709Z",
     "start_time": "2021-01-17T12:52:54.979313Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    hidden3 = tf.compat.v1.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name='hidden3')\n",
    "    hidden4 = tf.compat.v1.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4')\n",
    "    hidden5 = tf.compat.v1.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name='hidden5')\n",
    "    logits = tf.compat.v1.layers.dense(hidden5, n_outputs, name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:24.466109Z",
     "start_time": "2021-01-17T12:52:55.165377Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.5774\n",
      "1 Validation accuracy: 0.8184\n",
      "2 Validation accuracy: 0.875\n",
      "3 Validation accuracy: 0.8914\n",
      "4 Validation accuracy: 0.9018\n",
      "5 Validation accuracy: 0.9182\n",
      "6 Validation accuracy: 0.9232\n",
      "7 Validation accuracy: 0.9284\n",
      "8 Validation accuracy: 0.9346\n",
      "9 Validation accuracy: 0.9376\n",
      "10 Validation accuracy: 0.94\n",
      "11 Validation accuracy: 0.9446\n",
      "12 Validation accuracy: 0.9478\n",
      "13 Validation accuracy: 0.9498\n",
      "14 Validation accuracy: 0.9524\n",
      "15 Validation accuracy: 0.9548\n",
      "16 Validation accuracy: 0.9578\n",
      "17 Validation accuracy: 0.958\n",
      "18 Validation accuracy: 0.961\n",
      "19 Validation accuracy: 0.9614\n"
     ]
    }
   ],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reutilizando um modelo TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega a estrutura do grafo e lista as operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:06:29.248601Z",
     "start_time": "2021-01-17T13:06:29.178286Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "saver = tf.compat.v1.train.import_meta_graph('./my_model_final.ckpt.meta')\n",
    "\n",
    "for op in tf.compat.v1.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega as operações que serão utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:24.722487Z",
     "start_time": "2021-01-17T12:53:24.592609Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.compat.v1.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.compat.v1.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "accuracy = tf.compat.v1.get_default_graph().get_tensor_by_name('eval/accuracy:0')\n",
    "training_op = tf.compat.v1.get_default_graph().get_operation_by_name('GradientDescent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando uma coleção contendo todas as operações importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:24.832433Z",
     "start_time": "2021-01-17T12:53:24.725231Z"
    }
   },
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.compat.v1.add_to_collection('my_important_ops', op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:24.942422Z",
     "start_time": "2021-01-17T12:53:24.835326Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.compat.v1.get_collection('my_important_ops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicia uma sessão, restaura o estado do modelo e continua treinando em seus dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:53.980065Z",
     "start_time": "2021-01-17T12:53:24.946977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9622\n",
      "1 Validation accuracy: 0.9636\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.9628\n",
      "4 Validation accuracy: 0.9654\n",
      "5 Validation accuracy: 0.9644\n",
      "6 Validation accuracy: 0.9674\n",
      "7 Validation accuracy: 0.9672\n",
      "8 Validation accuracy: 0.967\n",
      "9 Validation accuracy: 0.9692\n",
      "10 Validation accuracy: 0.97\n",
      "11 Validation accuracy: 0.9694\n",
      "12 Validation accuracy: 0.9694\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9702\n",
      "15 Validation accuracy: 0.971\n",
      "16 Validation accuracy: 0.9704\n",
      "17 Validation accuracy: 0.9702\n",
      "18 Validation accuracy: 0.972\n",
      "19 Validation accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionamos uma nova 4ª camada oculta no topo da 3ª camada pré-treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:53:54.250158Z",
     "start_time": "2021-01-17T12:53:53.985448Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_hidden4 = 20  #nova camada\n",
    "n_outputs = 10  #nova camada\n",
    "\n",
    "saver = tf.compat.v1.train.import_meta_graph('./my_model_final.ckpt.meta')\n",
    "\n",
    "X = tf.compat.v1.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.compat.v1.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "hidden3 = tf.compat.v1.get_default_graph().get_tensor_by_name('dnn/hidden3/Relu:0')\n",
    "\n",
    "new_hidden4 = tf.compat.v1.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='new_hidden4')\n",
    "new_logits = tf.compat.v1.layers.dense(new_hidden4, n_outputs, name='new_outputs')\n",
    "\n",
    "with tf.name_scope('new_loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('new_eval'):\n",
    "    correct = tf.nn.in_top_k(y, new_logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "with tf.name_scope('new_train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "new_saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:54:20.677712Z",
     "start_time": "2021-01-17T12:53:54.252593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8924\n",
      "1 Validation accuracy: 0.9258\n",
      "2 Validation accuracy: 0.9388\n",
      "3 Validation accuracy: 0.944\n",
      "4 Validation accuracy: 0.9488\n",
      "5 Validation accuracy: 0.9506\n",
      "6 Validation accuracy: 0.9538\n",
      "7 Validation accuracy: 0.9586\n",
      "8 Validation accuracy: 0.9578\n",
      "9 Validation accuracy: 0.9618\n",
      "10 Validation accuracy: 0.9608\n",
      "11 Validation accuracy: 0.9626\n",
      "12 Validation accuracy: 0.9638\n",
      "13 Validation accuracy: 0.9654\n",
      "14 Validation accuracy: 0.9656\n",
      "15 Validation accuracy: 0.9642\n",
      "16 Validation accuracy: 0.9662\n",
      "17 Validation accuracy: 0.9672\n",
      "18 Validation accuracy: 0.9676\n",
      "19 Validation accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congelando as camadas inferiores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:54:36.830973Z",
     "start_time": "2021-01-17T12:54:20.680913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8948\n",
      "1 Validation accuracy: 0.9346\n",
      "2 Validation accuracy: 0.9414\n",
      "3 Validation accuracy: 0.945\n",
      "4 Validation accuracy: 0.9472\n",
      "5 Validation accuracy: 0.9494\n",
      "6 Validation accuracy: 0.9498\n",
      "7 Validation accuracy: 0.9504\n",
      "8 Validation accuracy: 0.951\n",
      "9 Validation accuracy: 0.951\n",
      "10 Validation accuracy: 0.9524\n",
      "11 Validation accuracy: 0.9536\n",
      "12 Validation accuracy: 0.9536\n",
      "13 Validation accuracy: 0.956\n",
      "14 Validation accuracy: 0.9548\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9562\n",
      "17 Validation accuracy: 0.9564\n",
      "18 Validation accuracy: 0.9566\n",
      "19 Validation accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300 #reúso\n",
    "n_hidden2 = 50  #reúso\n",
    "n_hidden3 = 50  #reúso\n",
    "n_hidden4 = 20  #nova camada\n",
    "n_outputs = 10  #nova camada\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')       #reúso\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2') #reúso\n",
    "    hidden3 = tf.compat.v1.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name='hidden3') #reúso\n",
    "    hidden4 = tf.compat.v1.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4') #nova camada\n",
    "    logits = tf.compat.v1.layers.dense(hidden4, n_outputs, name='outputs') #nova camada\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|outputs')\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "\n",
    "reuse_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='hidden[123]')\n",
    "restore_saver = tf.compat.v1.train.Saver(reuse_vars) # restaurando as camadas 1-3\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com tf.compat.v1.stop_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:54:52.935946Z",
     "start_time": "2021-01-17T12:54:36.833903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8804\n",
      "1 Validation accuracy: 0.9094\n",
      "2 Validation accuracy: 0.9314\n",
      "3 Validation accuracy: 0.9368\n",
      "4 Validation accuracy: 0.9428\n",
      "5 Validation accuracy: 0.9464\n",
      "6 Validation accuracy: 0.948\n",
      "7 Validation accuracy: 0.9492\n",
      "8 Validation accuracy: 0.9514\n",
      "9 Validation accuracy: 0.9518\n",
      "10 Validation accuracy: 0.9544\n",
      "11 Validation accuracy: 0.9552\n",
      "12 Validation accuracy: 0.9554\n",
      "13 Validation accuracy: 0.9552\n",
      "14 Validation accuracy: 0.957\n",
      "15 Validation accuracy: 0.9562\n",
      "16 Validation accuracy: 0.956\n",
      "17 Validation accuracy: 0.9562\n",
      "18 Validation accuracy: 0.9574\n",
      "19 Validation accuracy: 0.9586\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1') #reutilizada e congelada\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2') #reutilizada e congelada\n",
    "    hidden2_stop = tf.compat.v1.stop_gradient(hidden2) #realiza o congelamento\n",
    "    hidden3 = tf.compat.v1.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name='hidden3') #reutilizada e congelada\n",
    "    hidden4 = tf.compat.v1.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4') #nova\n",
    "    logits = tf.compat.v1.layers.dense(hidden4, n_outputs, name='outputs') #nova\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|outputs')\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "\n",
    "reuse_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='hidden[123]')\n",
    "restore_saver = tf.compat.v1.train.Saver(reuse_vars) # restaurando as camadas 1-3\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenando em cache as camadas congeladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:54:53.194773Z",
     "start_time": "2021-01-17T12:54:52.940430Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1') #reutilizada e congelada\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2') #reutilizada, congelada e em cache\n",
    "    hidden2_stop = tf.compat.v1.stop_gradient(hidden2) #realiza o congelamento\n",
    "    hidden3 = tf.compat.v1.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name='hidden3') #reutilizada e congelada\n",
    "    hidden4 = tf.compat.v1.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4') #nova\n",
    "    logits = tf.compat.v1.layers.dense(hidden4, n_outputs, name='outputs') #nova\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|outputs')\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "\n",
    "reuse_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='hidden[123]')\n",
    "restore_saver = tf.compat.v1.train.Saver(reuse_vars) # restaurando as camadas 1-3\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:01.793075Z",
     "start_time": "2021-01-17T12:54:53.197382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8894\n",
      "1 Validation accuracy: 0.9264\n",
      "2 Validation accuracy: 0.9374\n",
      "3 Validation accuracy: 0.9434\n",
      "4 Validation accuracy: 0.9454\n",
      "5 Validation accuracy: 0.9496\n",
      "6 Validation accuracy: 0.95\n",
      "7 Validation accuracy: 0.9522\n",
      "8 Validation accuracy: 0.9522\n",
      "9 Validation accuracy: 0.9534\n",
      "10 Validation accuracy: 0.953\n",
      "11 Validation accuracy: 0.9554\n",
      "12 Validation accuracy: 0.9564\n",
      "13 Validation accuracy: 0.9548\n",
      "14 Validation accuracy: 0.9572\n",
      "15 Validation accuracy: 0.956\n",
      "16 Validation accuracy: 0.9562\n",
      "17 Validation accuracy: 0.957\n",
      "18 Validation accuracy: 0.9566\n",
      "19 Validation accuracy: 0.9568\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_model_final.ckpt')\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "    save_path = saver.save(sess, './my_new_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cronograma de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:02.045055Z",
     "start_time": "2021-01-17T12:55:01.798392Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    logits = tf.compat.v1.layers.dense(hidden2, n_outputs, name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:15.798163Z",
     "start_time": "2021-01-17T12:55:02.047445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.959\n",
      "1 Validation accuracy: 0.9716\n",
      "2 Validation accuracy: 0.9778\n",
      "3 Validation accuracy: 0.9796\n",
      "4 Validation accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evitando overfitting por meio da regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularização l1 e l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:16.031138Z",
     "start_time": "2021-01-17T12:55:15.801501Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "  \n",
    "scale = 0.001 # hiperparâmetro regularização l1\n",
    "\n",
    "my_dense_layer = partial(tf.compat.v1.layers.dense, activation=tf.nn.relu, kernel_regularizer=l1(l=scale))\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name='hidden1')\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name='hidden2')\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name='outputs')\n",
    "    \n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name='avg_xentropy')\n",
    "    reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name='loss')\n",
    "\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:42.434700Z",
     "start_time": "2021-01-17T12:55:16.033546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8076\n",
      "1 Validation accuracy: 0.8654\n",
      "2 Validation accuracy: 0.885\n",
      "3 Validation accuracy: 0.8936\n",
      "4 Validation accuracy: 0.9022\n",
      "5 Validation accuracy: 0.9062\n",
      "6 Validation accuracy: 0.9106\n",
      "7 Validation accuracy: 0.9106\n",
      "8 Validation accuracy: 0.9124\n",
      "9 Validation accuracy: 0.9146\n",
      "10 Validation accuracy: 0.9168\n",
      "11 Validation accuracy: 0.917\n",
      "12 Validation accuracy: 0.9174\n",
      "13 Validation accuracy: 0.9176\n",
      "14 Validation accuracy: 0.9182\n",
      "15 Validation accuracy: 0.9178\n",
      "16 Validation accuracy: 0.9182\n",
      "17 Validation accuracy: 0.9198\n",
      "18 Validation accuracy: 0.9184\n",
      "19 Validation accuracy: 0.9188\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:55:42.941396Z",
     "start_time": "2021-01-17T12:55:42.438039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-019e8a8b3420>:8: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "training = tf.compat.v1.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  #== 1 - keep_prob\n",
    "X_drop = tf.compat.v1.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden1_drop = tf.compat.v1.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    hidden2_drop = tf.compat.v1.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.compat.v1.layers.dense(hidden2_drop, n_outputs, name='outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:57:02.167525Z",
     "start_time": "2021-01-17T12:55:42.944069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9238\n",
      "1 Validation accuracy: 0.9448\n",
      "2 Validation accuracy: 0.951\n",
      "3 Validation accuracy: 0.9598\n",
      "4 Validation accuracy: 0.9604\n",
      "5 Validation accuracy: 0.9622\n",
      "6 Validation accuracy: 0.964\n",
      "7 Validation accuracy: 0.9648\n",
      "8 Validation accuracy: 0.9676\n",
      "9 Validation accuracy: 0.9668\n",
      "10 Validation accuracy: 0.9702\n",
      "11 Validation accuracy: 0.9704\n",
      "12 Validation accuracy: 0.97\n",
      "13 Validation accuracy: 0.9736\n",
      "14 Validation accuracy: 0.9722\n",
      "15 Validation accuracy: 0.9722\n",
      "16 Validation accuracy: 0.9728\n",
      "17 Validation accuracy: 0.9742\n",
      "18 Validation accuracy: 0.9726\n",
      "19 Validation accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularização Max-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:57:02.178809Z",
     "start_time": "2021-01-17T12:57:02.171026Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name='max_norm', collection='max_norm'):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.compat.v1.assign(weights, clipped, name=name)\n",
    "        tf.compat.v1.add_to_collection(collection, clip_weights)\n",
    "        return None\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:57:02.614244Z",
     "start_time": "2021-01-17T12:57:02.182546Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.compat.v1.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name='hidden1')\n",
    "    hidden2 = tf.compat.v1.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name='hidden2')\n",
    "    logits = tf.compat.v1.layers.dense(hidden2, n_outputs, name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(y, logits, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:58:20.027991Z",
     "start_time": "2021-01-17T12:57:02.616996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9598\n",
      "1 Validation accuracy: 0.9708\n",
      "2 Validation accuracy: 0.978\n",
      "3 Validation accuracy: 0.9756\n",
      "4 Validation accuracy: 0.9772\n",
      "5 Validation accuracy: 0.9806\n",
      "6 Validation accuracy: 0.979\n",
      "7 Validation accuracy: 0.9816\n",
      "8 Validation accuracy: 0.9806\n",
      "9 Validation accuracy: 0.9826\n",
      "10 Validation accuracy: 0.9844\n",
      "11 Validation accuracy: 0.9844\n",
      "12 Validation accuracy: 0.9822\n",
      "13 Validation accuracy: 0.984\n",
      "14 Validation accuracy: 0.986\n",
      "15 Validation accuracy: 0.9816\n",
      "16 Validation accuracy: 0.9838\n",
      "17 Validation accuracy: 0.9848\n",
      "18 Validation accuracy: 0.9846\n",
      "19 Validation accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "clip_all_weights = tf.compat.v1.get_collection('max_norm')\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, 'Validation accuracy:', acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercícios do capítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*8*\n",
    "\n",
    "Exercício: Construa um DNN com cinco camadas ocultas de 100 neurônios cada, inicialização de He e a função de ativação de ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:10:09.435374Z",
     "start_time": "2021-01-18T10:10:09.420150Z"
    }
   },
   "outputs": [],
   "source": [
    "he_init = tf.compat.v1.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.compat.v1.variable_scope(name, 'dnn'):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.compat.v1.layers.dense(inputs, n_neurons, activation=activation, kernel_initializer=initializer, \n",
    "                                               name='hidden%d' % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:11:27.831417Z",
     "start_time": "2021-01-18T10:10:14.660805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-9a3330ed1d81>:7: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/liliane-hop/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/liliane-hop/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "0\tValidation loss: 0.152794\tBest loss: 0.152794\tAccuracy: 95.74%\n",
      "1\tValidation loss: 1.214428\tBest loss: 0.152794\tAccuracy: 96.40%\n",
      "2\tValidation loss: 0.079058\tBest loss: 0.079058\tAccuracy: 97.93%\n",
      "3\tValidation loss: 0.097267\tBest loss: 0.079058\tAccuracy: 97.69%\n",
      "4\tValidation loss: 0.105297\tBest loss: 0.079058\tAccuracy: 97.65%\n",
      "5\tValidation loss: 0.132889\tBest loss: 0.079058\tAccuracy: 97.15%\n",
      "6\tValidation loss: 0.133640\tBest loss: 0.079058\tAccuracy: 97.34%\n",
      "7\tValidation loss: 0.167731\tBest loss: 0.079058\tAccuracy: 97.15%\n",
      "8\tValidation loss: 0.144273\tBest loss: 0.079058\tAccuracy: 97.69%\n",
      "9\tValidation loss: 0.279859\tBest loss: 0.079058\tAccuracy: 94.68%\n",
      "10\tValidation loss: 0.383678\tBest loss: 0.079058\tAccuracy: 93.35%\n",
      "11\tValidation loss: 0.133592\tBest loss: 0.079058\tAccuracy: 98.36%\n",
      "12\tValidation loss: 0.087782\tBest loss: 0.079058\tAccuracy: 98.24%\n",
      "13\tValidation loss: 0.157156\tBest loss: 0.079058\tAccuracy: 97.93%\n",
      "14\tValidation loss: 0.099544\tBest loss: 0.079058\tAccuracy: 98.05%\n",
      "15\tValidation loss: 0.426598\tBest loss: 0.079058\tAccuracy: 92.46%\n",
      "16\tValidation loss: 0.384472\tBest loss: 0.079058\tAccuracy: 92.57%\n",
      "17\tValidation loss: 0.202266\tBest loss: 0.079058\tAccuracy: 94.72%\n",
      "18\tValidation loss: 0.260788\tBest loss: 0.079058\tAccuracy: 94.88%\n",
      "19\tValidation loss: 0.281330\tBest loss: 0.079058\tAccuracy: 93.94%\n",
      "20\tValidation loss: 1.024717\tBest loss: 0.079058\tAccuracy: 92.26%\n",
      "21\tValidation loss: 0.686913\tBest loss: 0.079058\tAccuracy: 87.65%\n",
      "22\tValidation loss: 0.940029\tBest loss: 0.079058\tAccuracy: 65.64%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 98.31%\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_outputs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "logits = tf.compat.v1.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name='logits')\n",
    "Y_proba = tf.nn.softmax(logits, name='Y_proba')\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name='training_op')\n",
    "\n",
    "correct = tf.nn.in_top_k(y, logits, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, './my_mnist_model_0_to_4.ckpt')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "        \n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, './my_mnist_model_0_to_4.ckpt')\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print('Final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:29:28.706528Z",
     "start_time": "2021-01-18T10:29:28.637426Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.compat.v1.train.AdamOptimizer, learning_rate=0.01, \n",
    "                 batch_size=20, activation=tf.nn.elu, initializer=he_init, batch_norm_momentum=None, dropout_rate=None, \n",
    "                 random_state=None):\n",
    "        \"\"\"Inicialize o DNNClassifier simplesmente armazenando todos os hiperparâmetros.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Construa as camadas ocultas, com suporte para normalização e eliminação de lote.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.compat.v1.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.compat.v1.layers.dense(inputs, self.n_neurons, kernel_initializer=self.initializer, \n",
    "                                               name='hidden%d' % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.compat.v1.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum, training=self._training)\n",
    "            inputs = self.activation(inputs, name='hidden%d_out' % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Construa o mesmo modelo anterior\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.compat.v1.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.compat.v1.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.compat.v1.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name='logits')\n",
    "        Y_proba = tf.nn.softmax(logits, name='Y_proba')\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(y, logits, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        # Disponibilize operações importantes facilmente por meio de variáveis de instância\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Obtenha todos os valores de variáveis (usados para parar antecipadamente, mais rápido do que salvar no disco)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Defina todas as variáveis para os valores fornecidos (para parada antecipada, mais rápido do que carregar do disco)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\") for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Ajuste o modelo ao conjunto de treinamento. Se X_valid e y_valid forem fornecidos, use a parada antecipada.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        #inferir n_inputs e n_outputs do conjunto de treinamento.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        #Traduza o vetor de rótulos para um vetor de índices de classe classificados, contendo inteiros de 0 a n_outputs - 1.\n",
    "        self.class_to_index_ = {label: index for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label] for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.compat.v1.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            #operações extras para normalização em lote\n",
    "            extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        #necessário em caso de parada precoce\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        self._session = tf.compat.v1.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy], feed_dict={self._X: X_valid, self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print('Early stopping!')\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy], feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "                    print('{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_train, acc_train * 100))\n",
    "            #Se usarmos a parada antecipada, reverta para o melhor modelo encontrado\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError('This %s instance is not fitted yet' % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]] for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T11:53:43.932485Z",
     "start_time": "2021-01-18T10:29:29.313400Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    'n_neurons': [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    'batch_size': [10, 50, 100, 500],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'activation': [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50, cv=3, random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:02:53.421522Z",
     "start_time": "2021-01-18T12:02:53.381475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 120, 'learning_rate': 0.01, 'batch_size': 500, 'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fb68d78ee60>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9933839268340144"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(rnd_search.best_params_)\n",
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:02:56.072373Z",
     "start_time": "2021-01-18T12:02:55.842020Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_.save('./my_best_mnist_model_0_to_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização em lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:03:47.828777Z",
     "start_time": "2021-01-18T12:02:57.104537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-a78388c09c28>:29: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "0\tValidation loss: 0.050460\tBest loss: 0.050460\tAccuracy: 98.48%\n",
      "1\tValidation loss: 0.045289\tBest loss: 0.045289\tAccuracy: 98.48%\n",
      "2\tValidation loss: 0.035835\tBest loss: 0.035835\tAccuracy: 98.75%\n",
      "3\tValidation loss: 0.045831\tBest loss: 0.035835\tAccuracy: 98.79%\n",
      "4\tValidation loss: 0.038656\tBest loss: 0.035835\tAccuracy: 98.75%\n",
      "5\tValidation loss: 0.041875\tBest loss: 0.035835\tAccuracy: 98.87%\n",
      "6\tValidation loss: 0.033227\tBest loss: 0.033227\tAccuracy: 99.02%\n",
      "7\tValidation loss: 0.043663\tBest loss: 0.033227\tAccuracy: 98.83%\n",
      "8\tValidation loss: 0.049093\tBest loss: 0.033227\tAccuracy: 98.79%\n",
      "9\tValidation loss: 0.035765\tBest loss: 0.033227\tAccuracy: 99.06%\n",
      "10\tValidation loss: 0.038398\tBest loss: 0.033227\tAccuracy: 98.98%\n",
      "11\tValidation loss: 0.032791\tBest loss: 0.032791\tAccuracy: 99.22%\n",
      "12\tValidation loss: 0.046320\tBest loss: 0.032791\tAccuracy: 98.98%\n",
      "13\tValidation loss: 0.032912\tBest loss: 0.032791\tAccuracy: 99.18%\n",
      "14\tValidation loss: 0.048663\tBest loss: 0.032791\tAccuracy: 98.83%\n",
      "15\tValidation loss: 0.032740\tBest loss: 0.032740\tAccuracy: 99.34%\n",
      "16\tValidation loss: 0.029918\tBest loss: 0.029918\tAccuracy: 99.22%\n",
      "17\tValidation loss: 0.036582\tBest loss: 0.029918\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.044790\tBest loss: 0.029918\tAccuracy: 98.91%\n",
      "19\tValidation loss: 0.033856\tBest loss: 0.029918\tAccuracy: 99.10%\n",
      "20\tValidation loss: 0.032623\tBest loss: 0.029918\tAccuracy: 99.34%\n",
      "21\tValidation loss: 0.031907\tBest loss: 0.029918\tAccuracy: 99.26%\n",
      "22\tValidation loss: 0.032402\tBest loss: 0.029918\tAccuracy: 99.22%\n",
      "23\tValidation loss: 0.021041\tBest loss: 0.021041\tAccuracy: 99.34%\n",
      "24\tValidation loss: 0.043237\tBest loss: 0.021041\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.032175\tBest loss: 0.021041\tAccuracy: 99.26%\n",
      "26\tValidation loss: 0.034561\tBest loss: 0.021041\tAccuracy: 99.06%\n",
      "27\tValidation loss: 0.029373\tBest loss: 0.021041\tAccuracy: 99.41%\n",
      "28\tValidation loss: 0.028197\tBest loss: 0.021041\tAccuracy: 99.41%\n",
      "29\tValidation loss: 0.032315\tBest loss: 0.021041\tAccuracy: 99.22%\n",
      "30\tValidation loss: 0.028851\tBest loss: 0.021041\tAccuracy: 99.30%\n",
      "31\tValidation loss: 0.038939\tBest loss: 0.021041\tAccuracy: 98.98%\n",
      "32\tValidation loss: 0.049233\tBest loss: 0.021041\tAccuracy: 98.51%\n",
      "33\tValidation loss: 0.036686\tBest loss: 0.021041\tAccuracy: 99.06%\n",
      "34\tValidation loss: 0.037261\tBest loss: 0.021041\tAccuracy: 99.02%\n",
      "35\tValidation loss: 0.037627\tBest loss: 0.021041\tAccuracy: 98.94%\n",
      "36\tValidation loss: 0.036512\tBest loss: 0.021041\tAccuracy: 99.10%\n",
      "37\tValidation loss: 0.032639\tBest loss: 0.021041\tAccuracy: 99.41%\n",
      "38\tValidation loss: 0.037404\tBest loss: 0.021041\tAccuracy: 99.18%\n",
      "39\tValidation loss: 0.039220\tBest loss: 0.021041\tAccuracy: 99.14%\n",
      "40\tValidation loss: 0.030837\tBest loss: 0.021041\tAccuracy: 99.18%\n",
      "41\tValidation loss: 0.026409\tBest loss: 0.021041\tAccuracy: 99.37%\n",
      "42\tValidation loss: 0.028186\tBest loss: 0.021041\tAccuracy: 99.37%\n",
      "43\tValidation loss: 0.022518\tBest loss: 0.021041\tAccuracy: 99.41%\n",
      "44\tValidation loss: 0.021986\tBest loss: 0.021041\tAccuracy: 99.57%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fb5d616cf80>,\n",
       "              batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x7fb68d23c850>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01, n_neurons=90, random_state=42, \n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:03:48.165804Z",
     "start_time": "2021-01-18T12:03:47.899924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922163845106052"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:04:27.267373Z",
     "start_time": "2021-01-18T12:03:48.236054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-a78388c09c28>:25: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "0\tValidation loss: 0.141291\tBest loss: 0.141291\tAccuracy: 96.25%\n",
      "1\tValidation loss: 0.112565\tBest loss: 0.112565\tAccuracy: 97.15%\n",
      "2\tValidation loss: 0.092053\tBest loss: 0.092053\tAccuracy: 97.62%\n",
      "3\tValidation loss: 0.095979\tBest loss: 0.092053\tAccuracy: 98.08%\n",
      "4\tValidation loss: 0.095685\tBest loss: 0.092053\tAccuracy: 97.97%\n",
      "5\tValidation loss: 0.083253\tBest loss: 0.083253\tAccuracy: 98.20%\n",
      "6\tValidation loss: 0.083881\tBest loss: 0.083253\tAccuracy: 97.73%\n",
      "7\tValidation loss: 0.070322\tBest loss: 0.070322\tAccuracy: 98.40%\n",
      "8\tValidation loss: 0.070613\tBest loss: 0.070322\tAccuracy: 98.32%\n",
      "9\tValidation loss: 0.067979\tBest loss: 0.067979\tAccuracy: 98.28%\n",
      "10\tValidation loss: 0.077395\tBest loss: 0.067979\tAccuracy: 98.36%\n",
      "11\tValidation loss: 0.073393\tBest loss: 0.067979\tAccuracy: 98.36%\n",
      "12\tValidation loss: 0.067919\tBest loss: 0.067919\tAccuracy: 98.36%\n",
      "13\tValidation loss: 0.063031\tBest loss: 0.063031\tAccuracy: 98.32%\n",
      "14\tValidation loss: 0.061621\tBest loss: 0.061621\tAccuracy: 98.55%\n",
      "15\tValidation loss: 0.062637\tBest loss: 0.061621\tAccuracy: 98.51%\n",
      "16\tValidation loss: 0.062781\tBest loss: 0.061621\tAccuracy: 98.40%\n",
      "17\tValidation loss: 0.064645\tBest loss: 0.061621\tAccuracy: 98.51%\n",
      "18\tValidation loss: 0.070947\tBest loss: 0.061621\tAccuracy: 98.36%\n",
      "19\tValidation loss: 0.062682\tBest loss: 0.061621\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.070945\tBest loss: 0.061621\tAccuracy: 98.20%\n",
      "21\tValidation loss: 0.061014\tBest loss: 0.061014\tAccuracy: 98.59%\n",
      "22\tValidation loss: 0.075722\tBest loss: 0.061014\tAccuracy: 98.12%\n",
      "23\tValidation loss: 0.068381\tBest loss: 0.061014\tAccuracy: 98.28%\n",
      "24\tValidation loss: 0.074469\tBest loss: 0.061014\tAccuracy: 98.24%\n",
      "25\tValidation loss: 0.070472\tBest loss: 0.061014\tAccuracy: 97.97%\n",
      "26\tValidation loss: 0.076701\tBest loss: 0.061014\tAccuracy: 98.44%\n",
      "27\tValidation loss: 0.102892\tBest loss: 0.061014\tAccuracy: 97.81%\n",
      "28\tValidation loss: 0.129246\tBest loss: 0.061014\tAccuracy: 95.43%\n",
      "29\tValidation loss: 0.101764\tBest loss: 0.061014\tAccuracy: 96.99%\n",
      "30\tValidation loss: 0.085889\tBest loss: 0.061014\tAccuracy: 97.81%\n",
      "31\tValidation loss: 0.201141\tBest loss: 0.061014\tAccuracy: 94.68%\n",
      "32\tValidation loss: 0.436288\tBest loss: 0.061014\tAccuracy: 89.33%\n",
      "33\tValidation loss: 0.243618\tBest loss: 0.061014\tAccuracy: 91.44%\n",
      "34\tValidation loss: 0.203489\tBest loss: 0.061014\tAccuracy: 93.39%\n",
      "35\tValidation loss: 0.159979\tBest loss: 0.061014\tAccuracy: 94.64%\n",
      "36\tValidation loss: 0.172097\tBest loss: 0.061014\tAccuracy: 93.28%\n",
      "37\tValidation loss: 0.137196\tBest loss: 0.061014\tAccuracy: 96.76%\n",
      "38\tValidation loss: 0.138932\tBest loss: 0.061014\tAccuracy: 95.93%\n",
      "39\tValidation loss: 0.164604\tBest loss: 0.061014\tAccuracy: 94.72%\n",
      "40\tValidation loss: 0.141045\tBest loss: 0.061014\tAccuracy: 95.74%\n",
      "41\tValidation loss: 0.161183\tBest loss: 0.061014\tAccuracy: 93.82%\n",
      "42\tValidation loss: 0.117156\tBest loss: 0.061014\tAccuracy: 97.42%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7fb625922b90>,\n",
       "              batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x7fb68d23c850>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01, n_neurons=90, random_state=42, \n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:04:27.476851Z",
     "start_time": "2021-01-18T12:04:27.333531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9863786728935591"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*9*\n",
    "\n",
    "Exercício: crie um novo DNN que reutilize todas as camadas ocultas pré-treinadas do modelo anterior, congele-as e substitua a camada de saída do softmax por uma nova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:04.597037Z",
     "start_time": "2021-01-18T12:08:03.985495Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.compat.v1.train.import_meta_graph('./my_best_mnist_model_0_to_4.meta')\n",
    "\n",
    "X = tf.compat.v1.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.compat.v1.get_default_graph().get_tensor_by_name('y:0')\n",
    "loss = tf.compat.v1.get_default_graph().get_tensor_by_name('loss:0')\n",
    "Y_proba = tf.compat.v1.get_default_graph().get_tensor_by_name('Y_proba:0')\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.compat.v1.get_default_graph().get_tensor_by_name('accuracy:0')\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='logits')\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate, name='Adam2')\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, logits, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "five_frozen_saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5\n",
    "\n",
    "\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:15.267092Z",
     "start_time": "2021-01-18T12:08:11.539776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 1.323986\tBest loss: 1.323986\tAccuracy: 40.00%\n",
      "1\tValidation loss: 1.122158\tBest loss: 1.122158\tAccuracy: 55.33%\n",
      "2\tValidation loss: 1.087038\tBest loss: 1.087038\tAccuracy: 56.00%\n",
      "3\tValidation loss: 1.101062\tBest loss: 1.087038\tAccuracy: 56.67%\n",
      "4\tValidation loss: 1.082933\tBest loss: 1.082933\tAccuracy: 60.00%\n",
      "5\tValidation loss: 0.939973\tBest loss: 0.939973\tAccuracy: 64.67%\n",
      "6\tValidation loss: 0.974622\tBest loss: 0.939973\tAccuracy: 60.00%\n",
      "7\tValidation loss: 0.985967\tBest loss: 0.939973\tAccuracy: 60.67%\n",
      "8\tValidation loss: 0.944546\tBest loss: 0.939973\tAccuracy: 65.33%\n",
      "9\tValidation loss: 0.929460\tBest loss: 0.929460\tAccuracy: 63.33%\n",
      "10\tValidation loss: 1.005449\tBest loss: 0.929460\tAccuracy: 60.00%\n",
      "11\tValidation loss: 0.925195\tBest loss: 0.925195\tAccuracy: 64.00%\n",
      "12\tValidation loss: 0.951632\tBest loss: 0.925195\tAccuracy: 66.00%\n",
      "13\tValidation loss: 1.018969\tBest loss: 0.925195\tAccuracy: 60.00%\n",
      "14\tValidation loss: 0.945258\tBest loss: 0.925195\tAccuracy: 64.67%\n",
      "15\tValidation loss: 0.943180\tBest loss: 0.925195\tAccuracy: 68.00%\n",
      "16\tValidation loss: 0.906473\tBest loss: 0.906473\tAccuracy: 68.00%\n",
      "17\tValidation loss: 0.882045\tBest loss: 0.882045\tAccuracy: 70.00%\n",
      "18\tValidation loss: 0.921251\tBest loss: 0.882045\tAccuracy: 65.33%\n",
      "19\tValidation loss: 0.886835\tBest loss: 0.882045\tAccuracy: 68.67%\n",
      "20\tValidation loss: 0.929380\tBest loss: 0.882045\tAccuracy: 63.33%\n",
      "21\tValidation loss: 0.970232\tBest loss: 0.882045\tAccuracy: 61.33%\n",
      "22\tValidation loss: 0.894679\tBest loss: 0.882045\tAccuracy: 68.67%\n",
      "23\tValidation loss: 0.898856\tBest loss: 0.882045\tAccuracy: 70.00%\n",
      "24\tValidation loss: 0.872924\tBest loss: 0.872924\tAccuracy: 69.33%\n",
      "25\tValidation loss: 1.098330\tBest loss: 0.872924\tAccuracy: 59.33%\n",
      "26\tValidation loss: 0.967539\tBest loss: 0.872924\tAccuracy: 62.00%\n",
      "27\tValidation loss: 1.020252\tBest loss: 0.872924\tAccuracy: 61.33%\n",
      "28\tValidation loss: 0.864419\tBest loss: 0.864419\tAccuracy: 68.67%\n",
      "29\tValidation loss: 0.873116\tBest loss: 0.864419\tAccuracy: 66.00%\n",
      "30\tValidation loss: 0.837275\tBest loss: 0.837275\tAccuracy: 68.67%\n",
      "31\tValidation loss: 0.870386\tBest loss: 0.837275\tAccuracy: 64.67%\n",
      "32\tValidation loss: 0.962587\tBest loss: 0.837275\tAccuracy: 61.33%\n",
      "33\tValidation loss: 0.894395\tBest loss: 0.837275\tAccuracy: 62.67%\n",
      "34\tValidation loss: 0.870802\tBest loss: 0.837275\tAccuracy: 65.33%\n",
      "35\tValidation loss: 0.872910\tBest loss: 0.837275\tAccuracy: 68.00%\n",
      "36\tValidation loss: 0.837787\tBest loss: 0.837275\tAccuracy: 70.00%\n",
      "37\tValidation loss: 0.907191\tBest loss: 0.837275\tAccuracy: 65.33%\n",
      "38\tValidation loss: 0.884579\tBest loss: 0.837275\tAccuracy: 66.67%\n",
      "39\tValidation loss: 0.838632\tBest loss: 0.837275\tAccuracy: 67.33%\n",
      "40\tValidation loss: 0.862703\tBest loss: 0.837275\tAccuracy: 70.00%\n",
      "41\tValidation loss: 0.876316\tBest loss: 0.837275\tAccuracy: 68.67%\n",
      "42\tValidation loss: 0.849476\tBest loss: 0.837275\tAccuracy: 66.67%\n",
      "43\tValidation loss: 0.960156\tBest loss: 0.837275\tAccuracy: 60.00%\n",
      "44\tValidation loss: 0.932247\tBest loss: 0.837275\tAccuracy: 62.00%\n",
      "45\tValidation loss: 0.875171\tBest loss: 0.837275\tAccuracy: 67.33%\n",
      "46\tValidation loss: 0.894488\tBest loss: 0.837275\tAccuracy: 64.67%\n",
      "47\tValidation loss: 0.963892\tBest loss: 0.837275\tAccuracy: 64.00%\n",
      "48\tValidation loss: 0.933110\tBest loss: 0.837275\tAccuracy: 65.33%\n",
      "49\tValidation loss: 0.902508\tBest loss: 0.837275\tAccuracy: 65.33%\n",
      "50\tValidation loss: 0.866182\tBest loss: 0.837275\tAccuracy: 69.33%\n",
      "Early stopping!\n",
      "Total training time: 3.2s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 60.52%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_best_mnist_model_0_to_4')\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, './my_mnist_model_5_to_9_five_frozen')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('Total training time: {:.1f}s'.format(t1 - t0))\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, './my_mnist_model_5_to_9_five_frozen')\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print('Final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando as camadas congeladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:21.835480Z",
     "start_time": "2021-01-18T12:08:18.337138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 1.313203\tBest loss: 1.313203\tAccuracy: 42.67%\n",
      "1\tValidation loss: 1.137299\tBest loss: 1.137299\tAccuracy: 49.33%\n",
      "2\tValidation loss: 1.131988\tBest loss: 1.131988\tAccuracy: 55.33%\n",
      "3\tValidation loss: 0.980973\tBest loss: 0.980973\tAccuracy: 61.33%\n",
      "4\tValidation loss: 0.973064\tBest loss: 0.973064\tAccuracy: 65.33%\n",
      "5\tValidation loss: 0.966888\tBest loss: 0.966888\tAccuracy: 68.67%\n",
      "6\tValidation loss: 0.936609\tBest loss: 0.936609\tAccuracy: 62.67%\n",
      "7\tValidation loss: 1.027715\tBest loss: 0.936609\tAccuracy: 58.00%\n",
      "8\tValidation loss: 0.962835\tBest loss: 0.936609\tAccuracy: 65.33%\n",
      "9\tValidation loss: 0.967799\tBest loss: 0.936609\tAccuracy: 64.00%\n",
      "10\tValidation loss: 0.953981\tBest loss: 0.936609\tAccuracy: 60.67%\n",
      "11\tValidation loss: 0.918197\tBest loss: 0.918197\tAccuracy: 68.67%\n",
      "12\tValidation loss: 0.940966\tBest loss: 0.918197\tAccuracy: 66.67%\n",
      "13\tValidation loss: 0.906739\tBest loss: 0.906739\tAccuracy: 64.00%\n",
      "14\tValidation loss: 0.912643\tBest loss: 0.906739\tAccuracy: 65.33%\n",
      "15\tValidation loss: 0.955029\tBest loss: 0.906739\tAccuracy: 59.33%\n",
      "16\tValidation loss: 0.915535\tBest loss: 0.906739\tAccuracy: 66.00%\n",
      "17\tValidation loss: 0.901063\tBest loss: 0.901063\tAccuracy: 66.67%\n",
      "18\tValidation loss: 0.997730\tBest loss: 0.901063\tAccuracy: 63.33%\n",
      "19\tValidation loss: 0.983053\tBest loss: 0.901063\tAccuracy: 65.33%\n",
      "20\tValidation loss: 0.900043\tBest loss: 0.900043\tAccuracy: 68.00%\n",
      "21\tValidation loss: 0.910303\tBest loss: 0.900043\tAccuracy: 65.33%\n",
      "22\tValidation loss: 0.928909\tBest loss: 0.900043\tAccuracy: 69.33%\n",
      "23\tValidation loss: 0.872538\tBest loss: 0.872538\tAccuracy: 68.67%\n",
      "24\tValidation loss: 0.934293\tBest loss: 0.872538\tAccuracy: 62.67%\n",
      "25\tValidation loss: 0.934461\tBest loss: 0.872538\tAccuracy: 67.33%\n",
      "26\tValidation loss: 1.004177\tBest loss: 0.872538\tAccuracy: 64.00%\n",
      "27\tValidation loss: 0.903527\tBest loss: 0.872538\tAccuracy: 65.33%\n",
      "28\tValidation loss: 0.905716\tBest loss: 0.872538\tAccuracy: 65.33%\n",
      "29\tValidation loss: 0.905696\tBest loss: 0.872538\tAccuracy: 66.67%\n",
      "30\tValidation loss: 0.911134\tBest loss: 0.872538\tAccuracy: 66.00%\n",
      "31\tValidation loss: 0.910516\tBest loss: 0.872538\tAccuracy: 63.33%\n",
      "32\tValidation loss: 0.841597\tBest loss: 0.841597\tAccuracy: 69.33%\n",
      "33\tValidation loss: 0.957527\tBest loss: 0.841597\tAccuracy: 62.00%\n",
      "34\tValidation loss: 0.853582\tBest loss: 0.841597\tAccuracy: 68.67%\n",
      "35\tValidation loss: 0.919494\tBest loss: 0.841597\tAccuracy: 64.67%\n",
      "36\tValidation loss: 0.879122\tBest loss: 0.841597\tAccuracy: 68.67%\n",
      "37\tValidation loss: 0.852497\tBest loss: 0.841597\tAccuracy: 70.00%\n",
      "38\tValidation loss: 0.869958\tBest loss: 0.841597\tAccuracy: 64.00%\n",
      "39\tValidation loss: 0.893115\tBest loss: 0.841597\tAccuracy: 63.33%\n",
      "40\tValidation loss: 0.922792\tBest loss: 0.841597\tAccuracy: 66.00%\n",
      "41\tValidation loss: 0.910272\tBest loss: 0.841597\tAccuracy: 63.33%\n",
      "42\tValidation loss: 0.835073\tBest loss: 0.835073\tAccuracy: 69.33%\n",
      "43\tValidation loss: 0.910584\tBest loss: 0.835073\tAccuracy: 64.00%\n",
      "44\tValidation loss: 0.895915\tBest loss: 0.835073\tAccuracy: 65.33%\n",
      "45\tValidation loss: 1.017057\tBest loss: 0.835073\tAccuracy: 64.00%\n",
      "46\tValidation loss: 0.948930\tBest loss: 0.835073\tAccuracy: 60.00%\n",
      "47\tValidation loss: 0.869941\tBest loss: 0.835073\tAccuracy: 69.33%\n",
      "48\tValidation loss: 0.971018\tBest loss: 0.835073\tAccuracy: 60.67%\n",
      "49\tValidation loss: 0.872860\tBest loss: 0.835073\tAccuracy: 64.67%\n",
      "50\tValidation loss: 0.996189\tBest loss: 0.835073\tAccuracy: 62.00%\n",
      "51\tValidation loss: 0.897958\tBest loss: 0.835073\tAccuracy: 69.33%\n",
      "52\tValidation loss: 0.937993\tBest loss: 0.835073\tAccuracy: 68.00%\n",
      "53\tValidation loss: 0.966650\tBest loss: 0.835073\tAccuracy: 60.00%\n",
      "54\tValidation loss: 0.928048\tBest loss: 0.835073\tAccuracy: 64.00%\n",
      "55\tValidation loss: 0.895289\tBest loss: 0.835073\tAccuracy: 63.33%\n",
      "56\tValidation loss: 0.900909\tBest loss: 0.835073\tAccuracy: 69.33%\n",
      "57\tValidation loss: 0.879077\tBest loss: 0.835073\tAccuracy: 66.00%\n",
      "58\tValidation loss: 0.880112\tBest loss: 0.835073\tAccuracy: 65.33%\n",
      "59\tValidation loss: 1.066004\tBest loss: 0.835073\tAccuracy: 60.00%\n",
      "60\tValidation loss: 0.898220\tBest loss: 0.835073\tAccuracy: 67.33%\n",
      "61\tValidation loss: 0.912704\tBest loss: 0.835073\tAccuracy: 64.67%\n",
      "62\tValidation loss: 0.877744\tBest loss: 0.835073\tAccuracy: 69.33%\n",
      "Early stopping!\n",
      "Total training time: 3.0s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 61.51%\n"
     ]
    }
   ],
   "source": [
    "hidden5_out = tf.compat.v1.get_default_graph().get_tensor_by_name('hidden5_out:0')\n",
    "\n",
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_best_mnist_model_0_to_4')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, './my_mnist_model_5_to_9_five_frozen')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('Total training time: {:.1f}s'.format(t1 - t0))\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, './my_mnist_model_5_to_9_five_frozen')\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print('Final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reutilizando apenas 4 camadas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:28.706827Z",
     "start_time": "2021-01-18T12:08:24.968459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 1.301529\tBest loss: 1.301529\tAccuracy: 46.00%\n",
      "1\tValidation loss: 1.107385\tBest loss: 1.107385\tAccuracy: 50.67%\n",
      "2\tValidation loss: 0.988803\tBest loss: 0.988803\tAccuracy: 56.00%\n",
      "3\tValidation loss: 1.037649\tBest loss: 0.988803\tAccuracy: 58.00%\n",
      "4\tValidation loss: 0.935892\tBest loss: 0.935892\tAccuracy: 63.33%\n",
      "5\tValidation loss: 0.952884\tBest loss: 0.935892\tAccuracy: 64.00%\n",
      "6\tValidation loss: 0.934928\tBest loss: 0.934928\tAccuracy: 67.33%\n",
      "7\tValidation loss: 0.894156\tBest loss: 0.894156\tAccuracy: 65.33%\n",
      "8\tValidation loss: 0.906545\tBest loss: 0.894156\tAccuracy: 64.67%\n",
      "9\tValidation loss: 0.890085\tBest loss: 0.890085\tAccuracy: 67.33%\n",
      "10\tValidation loss: 0.901048\tBest loss: 0.890085\tAccuracy: 71.33%\n",
      "11\tValidation loss: 0.952151\tBest loss: 0.890085\tAccuracy: 64.00%\n",
      "12\tValidation loss: 0.865701\tBest loss: 0.865701\tAccuracy: 74.00%\n",
      "13\tValidation loss: 0.838020\tBest loss: 0.838020\tAccuracy: 70.00%\n",
      "14\tValidation loss: 0.854174\tBest loss: 0.838020\tAccuracy: 68.00%\n",
      "15\tValidation loss: 0.867453\tBest loss: 0.838020\tAccuracy: 70.67%\n",
      "16\tValidation loss: 0.936794\tBest loss: 0.838020\tAccuracy: 64.00%\n",
      "17\tValidation loss: 0.825032\tBest loss: 0.825032\tAccuracy: 72.00%\n",
      "18\tValidation loss: 0.862864\tBest loss: 0.825032\tAccuracy: 74.00%\n",
      "19\tValidation loss: 0.823608\tBest loss: 0.823608\tAccuracy: 69.33%\n",
      "20\tValidation loss: 0.839467\tBest loss: 0.823608\tAccuracy: 71.33%\n",
      "21\tValidation loss: 0.866437\tBest loss: 0.823608\tAccuracy: 63.33%\n",
      "22\tValidation loss: 0.857893\tBest loss: 0.823608\tAccuracy: 68.00%\n",
      "23\tValidation loss: 0.859168\tBest loss: 0.823608\tAccuracy: 70.00%\n",
      "24\tValidation loss: 0.881733\tBest loss: 0.823608\tAccuracy: 70.00%\n",
      "25\tValidation loss: 0.883482\tBest loss: 0.823608\tAccuracy: 66.00%\n",
      "26\tValidation loss: 0.835947\tBest loss: 0.823608\tAccuracy: 72.00%\n",
      "27\tValidation loss: 0.822594\tBest loss: 0.822594\tAccuracy: 70.00%\n",
      "28\tValidation loss: 0.804027\tBest loss: 0.804027\tAccuracy: 72.00%\n",
      "29\tValidation loss: 0.821685\tBest loss: 0.804027\tAccuracy: 69.33%\n",
      "30\tValidation loss: 0.785795\tBest loss: 0.785795\tAccuracy: 70.00%\n",
      "31\tValidation loss: 0.794002\tBest loss: 0.785795\tAccuracy: 71.33%\n",
      "32\tValidation loss: 0.853720\tBest loss: 0.785795\tAccuracy: 70.00%\n",
      "33\tValidation loss: 0.836802\tBest loss: 0.785795\tAccuracy: 72.00%\n",
      "34\tValidation loss: 0.791424\tBest loss: 0.785795\tAccuracy: 72.00%\n",
      "35\tValidation loss: 0.854974\tBest loss: 0.785795\tAccuracy: 70.67%\n",
      "36\tValidation loss: 0.842566\tBest loss: 0.785795\tAccuracy: 70.67%\n",
      "37\tValidation loss: 0.801957\tBest loss: 0.785795\tAccuracy: 69.33%\n",
      "38\tValidation loss: 0.791053\tBest loss: 0.785795\tAccuracy: 71.33%\n",
      "39\tValidation loss: 0.794821\tBest loss: 0.785795\tAccuracy: 72.00%\n",
      "40\tValidation loss: 0.868316\tBest loss: 0.785795\tAccuracy: 65.33%\n",
      "41\tValidation loss: 0.819470\tBest loss: 0.785795\tAccuracy: 70.00%\n",
      "42\tValidation loss: 0.806893\tBest loss: 0.785795\tAccuracy: 72.67%\n",
      "43\tValidation loss: 0.818469\tBest loss: 0.785795\tAccuracy: 68.00%\n",
      "44\tValidation loss: 0.807464\tBest loss: 0.785795\tAccuracy: 71.33%\n",
      "45\tValidation loss: 0.860830\tBest loss: 0.785795\tAccuracy: 66.67%\n",
      "46\tValidation loss: 0.851027\tBest loss: 0.785795\tAccuracy: 66.00%\n",
      "47\tValidation loss: 0.841475\tBest loss: 0.785795\tAccuracy: 66.67%\n",
      "48\tValidation loss: 0.849200\tBest loss: 0.785795\tAccuracy: 69.33%\n",
      "49\tValidation loss: 0.795452\tBest loss: 0.785795\tAccuracy: 71.33%\n",
      "50\tValidation loss: 0.858837\tBest loss: 0.785795\tAccuracy: 65.33%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
      "Final test accuracy: 64.51%\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_outputs = 5\n",
    "\n",
    "restore_saver = tf.compat.v1.train.import_meta_graph('./my_best_mnist_model_0_to_4.meta')\n",
    "\n",
    "X = tf.compat.v1.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.compat.v1.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "hidden4_out = tf.compat.v1.get_default_graph().get_tensor_by_name('hidden4_out:0')\n",
    "logits = tf.compat.v1.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name='new_logits')\n",
    "\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, logits, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='new_logits')\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate, name='Adam2')\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "four_frozen_saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_best_mnist_model_0_to_4')\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, './my_mnist_model_5_to_9_four_frozen')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, './my_mnist_model_5_to_9_four_frozen')\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print('Final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descongelando as 2 camadas ocultas mais altas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:35.298901Z",
     "start_time": "2021-01-18T12:08:32.801175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen\n",
      "0\tValidation loss: 0.700471\tBest loss: 0.700471\tAccuracy: 75.33%\n",
      "1\tValidation loss: 0.771741\tBest loss: 0.700471\tAccuracy: 78.67%\n",
      "2\tValidation loss: 0.833554\tBest loss: 0.700471\tAccuracy: 79.33%\n",
      "3\tValidation loss: 0.832981\tBest loss: 0.700471\tAccuracy: 80.67%\n",
      "4\tValidation loss: 0.981024\tBest loss: 0.700471\tAccuracy: 77.33%\n",
      "5\tValidation loss: 0.857649\tBest loss: 0.700471\tAccuracy: 75.33%\n",
      "6\tValidation loss: 0.866220\tBest loss: 0.700471\tAccuracy: 78.67%\n",
      "7\tValidation loss: 0.892077\tBest loss: 0.700471\tAccuracy: 78.00%\n",
      "8\tValidation loss: 1.009419\tBest loss: 0.700471\tAccuracy: 77.33%\n",
      "9\tValidation loss: 1.002859\tBest loss: 0.700471\tAccuracy: 81.33%\n",
      "10\tValidation loss: 0.950551\tBest loss: 0.700471\tAccuracy: 82.00%\n",
      "11\tValidation loss: 0.982984\tBest loss: 0.700471\tAccuracy: 80.67%\n",
      "12\tValidation loss: 0.981532\tBest loss: 0.700471\tAccuracy: 82.67%\n",
      "13\tValidation loss: 0.908445\tBest loss: 0.700471\tAccuracy: 82.00%\n",
      "14\tValidation loss: 1.078972\tBest loss: 0.700471\tAccuracy: 82.67%\n",
      "15\tValidation loss: 1.001584\tBest loss: 0.700471\tAccuracy: 85.33%\n",
      "16\tValidation loss: 1.118467\tBest loss: 0.700471\tAccuracy: 83.33%\n",
      "17\tValidation loss: 1.132748\tBest loss: 0.700471\tAccuracy: 82.67%\n",
      "18\tValidation loss: 1.402663\tBest loss: 0.700471\tAccuracy: 78.00%\n",
      "19\tValidation loss: 1.363762\tBest loss: 0.700471\tAccuracy: 74.67%\n",
      "20\tValidation loss: 1.331901\tBest loss: 0.700471\tAccuracy: 78.67%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen\n",
      "Final test accuracy: 68.77%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|new_logits')\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate, name='Adam3')\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "two_frozen_saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, './my_mnist_model_5_to_9_four_frozen')\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = two_frozen_saver.save(sess, './my_mnist_model_5_to_9_two_frozen')\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        print('{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%'.format(epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, './my_mnist_model_5_to_9_two_frozen')\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print('Final test accuracy: {:.2f}%'.format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*10*\n",
    "\n",
    "Exercício: Comece construindo dois DNNs (vamos chamá-los de DNN A e B), ambos semelhantes ao que você construiu anteriormente, mas sem a camada de saída: cada DNN deve ter cinco camadas ocultas de 100 neurônios cada, inicialização de He e ativação de ELU. Em seguida, adicione mais uma camada oculta com 10 unidades no topo de ambos os DNNs. Você deve usar a função concat () do TensorFlow com axis = 1 para concatenar as saídas de ambos os DNNs ao longo do eixo horizontal e, em seguida, alimentar o resultado para a camada oculta. Finalmente, adicione uma camada de saída com um único neurônio usando a função de ativação logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:39.158701Z",
     "start_time": "2021-01-18T12:08:38.449226Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, 2, n_inputs), name='X')\n",
    "X1, X2 = tf.compat.v1.unstack(X, axis=1)\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "\n",
    "dnn1 = dnn(X1, name='DNN_A')\n",
    "dnn2 = dnn(X2, name='DNN_B')\n",
    "dnn_outputs = tf.compat.v1.concat([dnn1, dnn2], axis=1)\n",
    "\n",
    "\n",
    "hidden = tf.compat.v1.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.compat.v1.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
    "\n",
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:46.782314Z",
     "start_time": "2021-01-18T12:08:44.819427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2, 784), dtype('float32'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "\n",
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "X_batch.shape, X_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:08:47.290812Z",
     "start_time": "2021-01-18T12:08:46.876453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap='binary', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap='binary', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:26:09.505885Z",
     "start_time": "2021-01-18T12:08:51.135761Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, 'Train loss:', loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, 'Test accuracy:', acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, './my_digit_comparison_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarefa de classificação MNIST regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:33:23.688451Z",
     "start_time": "2021-01-18T12:32:53.518030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt\n",
      "0 Test accuracy: 0.9372\n",
      "10 Test accuracy: 0.9516\n",
      "20 Test accuracy: 0.9656\n",
      "30 Test accuracy: 0.9656\n",
      "40 Test accuracy: 0.9656\n",
      "50 Test accuracy: 0.9651\n",
      "60 Test accuracy: 0.9649\n",
      "70 Test accuracy: 0.9648\n",
      "80 Test accuracy: 0.9646\n",
      "90 Test accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "dnn_outputs = dnn(X, name='DNN_A')\n",
    "frozen_outputs = tf.compat.v1.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.compat.v1.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, logits, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='DNN_A')\n",
    "restore_saver = tf.compat.v1.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, './my_digit_comparison_model.ckpt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, 'Test accuracy:', acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, './my_mnist_model_final.ckpt')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
